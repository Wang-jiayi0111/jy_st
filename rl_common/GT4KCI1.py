import os
import re
import json
import torch
import random
import logging
import platform
import numpy as np
import networkx as nx
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from node2vec import Node2Vec
import matplotlib.cm as cm
import matplotlib.pyplot as plt
import matplotlib.colors as mcolors
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split, KFold
from sklearn.metrics import precision_recall_curve, average_precision_score
from pathlib import Path
from collections import Counter

BASE_PATH = "/home/ps/jy_exp/input_GNN"
# æ–°æ•°æ®é›†åç§°
NEW_DATASET = "gt4kciProject"
# é‡‡æ ·æ¯”ä¾‹ï¼ˆæ¯ä¸ªæ•°æ®é›†å–20%ï¼‰
SAMPLE_RATIO = 1.0
# è¦ç»„åˆçš„æ•°æ®é›†åˆ—è¡¨
DATASETS = ["JHotDraw"]

create_hybrid = True  # è®¾ä¸ºTrueæ—¶åˆ›å»ºæ··åˆæ•°æ®é›†
# æ˜¯å¦ä½¿ç”¨çœŸå®çš„æ•°æ®
is_real_data = True
# æ•°æ®é›†åç§°
if create_hybrid:
    dataset = "Hybrid"
else:
    dataset = "SPM"
# ç½‘ç»œç±»å‹ MCN(Multilayer Class Network) MPN(Multilayer Package Network)
net_type = "MCN"
# æ•°æ®è·¯å¾„  
real_data_path = f"/home/ps/jy_exp/input_GNN/{dataset}_{net_type}/combined_{dataset}_GN.net"
# æ˜¯å¦è¿›è¡Œå‚æ•°è°ƒä¼˜çš„
searching_best_param = False
# å…³é”®ç±»å æ¯”
key_class_percentage = 0.2
# å…³é”®ç±»æ•°é‡
num_key_nodes = 10
# æ˜¯å¦ä½¿ç”¨é¢„å®šä¹‰å…³é”®ç±» å¦‚æœä¸ºTrueåˆ™ä¸Šé¢çš„num_key_nodesè®¾ç½®ä¸ä¼šç”Ÿæ•ˆ
use_predefined_key_classes = False
# æ‰¾ä¸åˆ°ç²¾ç¡®åŒ¹é…æ—¶æ˜¯å¦å…è®¸æ¨¡ç³ŠåŒ¹é…
allow_fuzzy_matching = True
# æ²¡æœ‰æ‰¾åˆ°çš„å…³é”®ç±»æ˜¯å¦é€šè¿‡ä¸­å¿ƒæ€§è®¡ç®—æ¥è¡¥å……
auto_complete_missing_classes = True
# æ˜¯å¦ä½¿ç”¨ä¸Šæ¬¡è®­ç»ƒçš„æ¨¡å‹ç»§ç»­è®­ç»ƒ
use_previous_model = True
# æ˜¯å¦å¯ç”¨è¿ç§»å­¦ä¹ æ¨¡å¼
enable_transfer_learning = True
# æ˜¯å¦æ ‡å‡†åŒ–èŠ‚ç‚¹ç‰¹å¾
standardize_features = True
# æ˜¯å¦ä½¿ç”¨å¤šç§ä¸­å¿ƒæ€§æŒ‡æ ‡ä½œä¸ºç‰¹å¾
use_multiple_centrality = True
# è¿ç§»å­¦ä¹ è®¾ç½®
transfer_learning_settings = {
    'adaptive_learning_rate': True,    # è‡ªé€‚åº”å­¦ä¹ ç‡
    'gradual_unfreezing': True,        # æ¸è¿›å¼è§£å†»
    'feature_adaptation': True,        # ç‰¹å¾é€‚åº”å±‚
    'feature_weighting': True          # ç‰¹å¾æƒé‡
}
# é¢„å®šä¹‰çš„å…³é”®ç±»ï¼ˆä¸use_predefined_key_classesç›¸å…³ï¼‰
predefined_key_classes = {
    "ANT": ["ElementHandler", "IntrospectionHelper", "Main", "Project", "ProjectHelper",
            "RuntimeConfigurable", "Target", "Task", "TaskContainer", "UnknownElement"],
    "Hibernate": ["Column", "Configuration", "ConnectionProvider", "Criteria", "Criterion", "Projection", 
                  "Query", "Session", "SessionFactory", "SessionFactoryImplementor", "SessionImplementor", 
                  "Table", "Transaction", "Type"],
    "jEdit": ["Buffer", "EBMessage", "EditPane", "jEdit", "JEditTextArea", "Log", "View"],
    "JGAP": ["AveragingCrossoverOperator", "BaseGene", "BaseGeneticOperator", "BestChromosomesSelector", 
             "BooleanGene", "Chromosome", "Configuration", "CrossoverOperator", "DoubleGene", 
             "FitnessFunction", "FixedBinaryGene", "GaussianMutationOperator", "Genotype", 
             "IntegerGene", "MutationOperator", "NaturalSelector", "Population", "WeightedRouletteSelector"],
    "JHotDraw": ["CompositeFigure", "Drawing", "DrawingEditor", 
                 "Figure", "Handle", "StandardDrawingView", "DrawApplication", "DrawingView", "Tool"],
    "JMeter": ["AbstractAction", "JMeterEngine", "JMeterGUIComponent", "JMeterThread", "JMeterTreeModel", 
               "PreCompiler", "Sampler", "SampleResult", "TestCompiler", "TestElement", "TestListener", 
               "TestPlan", "TestPlanGui", "ThreadGroup"],
    "Log4j": ["Appender", "Configuration", "Filter", "Layout", "Logger", "LoggerConfig", 
              "LoggerContext", "StrLookup", "StrSubstitutor"],
    "Wro4J": ["Group", "Resource", "ResourcePostProcessor", "ResourcePreProcessor", "ResourceType", 
              "UriLocator", "UriLocatorFactory", "WroFilter", "WroManager", "WroManagerFactory", 
              "WroModel", "WroModelFactory"]
}

# è®­ç»ƒä¿¡æ¯è¾“å‡ºé¢‘ç‡
output_frequency = 5
# é¢„è®¾çš„æœ€ä½³å‚æ•°
preset_best_param = {
    'learning_rate': 1e-3,  # å­¦ä¹ ç‡
    'num_sampled_neighbors': 10,# 20
    'num_layers': 1,            
    'dropout_rate': 0.1,        # 0.3
    'weight_decay': 0.0005,  # L2æ­£åˆ™åŒ–å‚æ•°
    'batch_norm': True
}
num_epochs = 0  # è®­ç»ƒè½®æ•°

# Node2Vec çš„ p å’Œ q å‚æ•° å’Œ çº¬åº¦å‚æ•°
node2vec_p_param = 0.25    # ä¿®æ”¹ä¸º0.5ï¼Œæ›´å¥½çš„å¹³è¡¡BFSå’ŒDFS
node2vec_q_param = 2    # ä¿®æ”¹ä¸º1.0ï¼Œä½¿ç”¨å¹³è¡¡çš„ç­–ç•¥
Node2Vec_dimensions = 32  # å¢åŠ åˆ°64ç»´ï¼Œæä¾›æ›´ä¸°å¯Œçš„ç‰¹å¾è¡¨ç¤º        ç»´åº¦åŒ¹é…ä¸ä¸Šçš„åŸå› 
# ä¸­å¿ƒæ€§è°ƒæ•´çš„å‚æ•°
ramdom_gama_beta = False
GKCI_gama = 6.2395
GKCI_beta = -13.8278

# æ—©åœå‚æ•°
early_stopping_patience = 1000
early_stopping_delta = 0.001

# æ•°æ®é›†åˆ’åˆ†æ¬¡æ•°
num_splits = 1

# -----------------------------------
# æ—¥å¿—é…ç½®
# -----------------------------------
# ç¡®ä¿ç»“æœç›®å½•å­˜åœ¨
result_dir = "/home/ps/jy_exp/output/GNN_res4"
os.makedirs(result_dir, exist_ok=True)

# å†å²æ¨¡å‹ä¿å­˜è·¯å¾„
historical_models_dir = "/home/ps/jy_exp/output/GNN_res4/historicalModels"
os.makedirs(historical_models_dir, exist_ok=True)

# è·å–ä¸‹ä¸€ä¸ªç»“æœæ–‡ä»¶çš„åºå·ï¼Œå‘½åä¸º result-1.txt, result-2.txt, ...
def get_next_result_file():
    log_dir = os.path.join(result_dir, "logs")
    os.makedirs(log_dir, exist_ok=True)
    existing_files = os.listdir(log_dir)
    pattern = re.compile(r'log-(\d+)\.txt')
    max_num = 0
    for filename in existing_files:
        match = pattern.match(filename)
        if match:
            num = int(match.group(1))
            if num > max_num:
                max_num = num
    return os.path.join(log_dir, f"log-{max_num + 1}.txt"), max_num + 1


result_file_path, train_id = get_next_result_file()


# é…ç½®æ—¥å¿—
logger = logging.getLogger()
logger.setLevel(logging.INFO)

# åˆ›å»ºæ ¼å¼å™¨
formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')

# åˆ›å»ºæ–‡ä»¶å¤„ç†å™¨
file_handler = logging.FileHandler(result_file_path, mode='w', encoding='utf-8')
file_handler.setLevel(logging.INFO)
file_handler.setFormatter(formatter)
logger.addHandler(file_handler)

# åˆ›å»ºæ§åˆ¶å°å¤„ç†å™¨
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)
console_handler.setFormatter(formatter)
logger.addHandler(console_handler)


# -----------------------------------
# 1. æ•°æ®å¤„ç†
# -----------------------------------

# ç±»ä¾èµ–ç½‘ç»œå»ºæ¨¡  æ„å»ºæœ‰å‘å›¾Gå’ŒIDåˆ°åç§°çš„æ˜ å°„ï¼ˆG, node_mappingï¼‰
def read_net_file(file_path):
    """
    è¯»å–å¹¶è§£æ .net æ–‡ä»¶ï¼Œè¿”å›ä¸€ä¸ªæœ‰å‘åŠ æƒå›¾å’ŒèŠ‚ç‚¹ç¼–å·åˆ°åç§°çš„æ˜ å°„ã€‚

    :param file_path: .net æ–‡ä»¶è·¯å¾„
    :return: (NetworkX DiGraph, dict) å›¾å’ŒèŠ‚ç‚¹ç¼–å·åˆ°åç§°çš„æ˜ å°„
    """
    G = nx.DiGraph()
    node_mapping = {}
    with open(file_path, 'r', encoding='utf-8') as f:
        lines = f.readlines()

    vertices_section = False
    arcs_section = False
    for line in lines:
        line = line.strip()
        if line.startswith('*Vertices'):
            vertices_section = True
            arcs_section = False
            num_vertices = int(line.split()[1])
            continue
        elif line.startswith('*Arcs'):
            vertices_section = False
            arcs_section = True
            continue
        elif vertices_section:
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–èŠ‚ç‚¹ç¼–å·å’Œåç§°
            match = re.match(r'(\d+)\s+"(.+)"', line)
            if match:
                node_id = int(match.group(1)) - 1  # å‡è®¾èŠ‚ç‚¹ç¼–å·ä»1å¼€å§‹ï¼Œè½¬æ¢ä¸ºä»0å¼€å§‹
                node_name = match.group(2)
                G.add_node(node_id, name=node_name)
                node_mapping[node_id] = node_name
        elif arcs_section:
            # æå–è¾¹ä¿¡æ¯
            parts = line.split()
            if len(parts) == 3:
                source = int(parts[0]) - 1  # è½¬æ¢ä¸ºä»0å¼€å§‹
                target = int(parts[1]) - 1
                weight = float(parts[2])
                G.add_edge(source, target, weight=weight)
    return G, node_mapping


# 1.1 ç”Ÿæˆç±»ä¾èµ–ç½‘ç»œ    æ ¹æ®æ•°æ®é›†-å’Œä¸Šé¢æ— åŒºåˆ«&è‡ªç”Ÿæ ·ä¾‹ï¼ˆG, node_mappingï¼‰
def generate_sample_graph():
    """
    ç”Ÿæˆä¸€ä¸ªæ ·ä¾‹ç±»ä¾èµ–ç½‘ç»œï¼ˆæœ‰å‘åŠ æƒå›¾ï¼‰ã€‚
    æ¯ä¸ªèŠ‚ç‚¹ä»£è¡¨ä¸€ä¸ªç±»ï¼Œè¾¹ä»£è¡¨ç±»ä¹‹é—´çš„ä¾èµ–å…³ç³»ï¼Œæƒé‡è¡¨ç¤ºä¾èµ–å¼ºåº¦ã€‚
    """
    if create_hybrid:
        G, node_mapping = read_net_file(combined_net_file)
        return G, node_mapping

    if is_real_data:
        G, node_mapping = read_net_file(real_data_path)
        return G, node_mapping

    G = nx.DiGraph()
    num_nodes = 20
    G.add_nodes_from(range(num_nodes))

    # æ·»åŠ æœ‰å‘è¾¹åŠå…¶æƒé‡
    edges = [
        (0, 1, 3), (0, 2, 1), (0, 3, 2),
        (1, 4, 2), (1, 5, 3),
        (2, 5, 2), (2, 6, 4),
        (3, 6, 1), (3, 7, 3),
        (4, 8, 2), (5, 8, 1), (5, 9, 4),
        (6, 9, 2), (6, 10, 3),
        (7, 10, 1), (7, 11, 4),
        (8, 12, 2), (9, 12, 3), (10, 13, 2),
        (11, 13, 1), (11, 14, 3),
        (12, 15, 4), (13, 15, 2), (14, 15, 1),
        (15, 16, 3), (16, 17, 2),
        (17, 18, 1), (18, 19, 5),
        (19, 0, 2),  # æ·»åŠ ç¯è·¯
        (4, 2, 1), (5, 3, 2),
        (7, 4, 1), (10, 5, 2),
        (12, 7, 3), (15, 10, 2),
    ]
    for u, v, w in edges:
        G.add_edge(u, v, weight=w)

    return G, None


# è·å–å…³é”®ç±»æ ‡ç­¾  è¿”å›æ ‡ç­¾æ•°ç»„å’Œå…³é”®èŠ‚ç‚¹åˆ—è¡¨ï¼ˆlabels[], key_nodes[]ï¼‰
def get_key_class_labels(G, node_mapping=None):
    """
    æ ¹æ®åº¦ä¸­å¿ƒæ€§å’Œä»‹æ•°ä¸­å¿ƒæ€§è®¡ç®—æ¯ä¸ªèŠ‚ç‚¹çš„ç»¼åˆä¸­å¿ƒæ€§ï¼Œ
    æˆ–è€…ä½¿ç”¨é¢„å®šä¹‰çš„å…³é”®ç±»ï¼Œå¹¶é€‰æ‹©ç›¸åº”èŠ‚ç‚¹ä½œä¸ºå…³é”®ç±»ã€‚

    :param G: NetworkX å›¾
    :param node_mapping: èŠ‚ç‚¹IDåˆ°èŠ‚ç‚¹åç§°çš„æ˜ å°„
    :return: æ ‡ç­¾æ•°ç»„ï¼ˆ1è¡¨ç¤ºå…³é”®ç±»ï¼Œ0è¡¨ç¤ºéå…³é”®ç±»ï¼‰ï¼Œå…³é”®ç±»èŠ‚ç‚¹åˆ—è¡¨
    """
    # åˆ›å»ºæ ‡ç­¾æ•°ç»„
    labels = np.zeros(len(G.nodes()))
    # # å¦‚æœæ˜¯æ··åˆæ•°æ®é›†ï¼Œä»æ–‡ä»¶åŠ è½½æ ‡ç­¾
    # if dataset == "Hybrid":
    #     label_path = real_data_path.replace(".net", "_labels.json")
    #     if os.path.exists(label_path):
    #         with open(label_path, 'r') as f:
    #             data = json.load(f)
    #             labels = data['labels']
    #             # è½¬æ¢ä¸ºnumpyæ•°ç»„
    #             labels = np.array(labels, dtype=int)
    #             # è·å–å…³é”®èŠ‚ç‚¹åˆ—è¡¨
    #             key_nodes = [node for node, label in enumerate(labels) if label == 1]
    #             return labels, key_nodes
    # ä½¿ç”¨é¢„å®šä¹‰çš„å…³é”®ç±»
    if use_predefined_key_classes and dataset in predefined_key_classes and node_mapping:
        key_class_names = predefined_key_classes[dataset]
        key_nodes = []
        not_found_classes = []
        
        # åˆ›å»ºä¸€ä¸ªåç§°åˆ°èŠ‚ç‚¹IDçš„æ˜ å°„
        name_to_node = {name: node_id for node_id, name in node_mapping.items()}
        
        # æ‰¾åˆ°å¯¹åº”çš„èŠ‚ç‚¹ID - æ›´ç²¾ç¡®çš„åŒ¹é…
        for class_name in key_class_names:
            matched = False
            for name, node_id in name_to_node.items():
                # è·å–å®Œæ•´ç±»åçš„æœ€åä¸€éƒ¨åˆ†ï¼ˆä¸å«åŒ…åï¼‰
                short_name = name.split('.')[-1]
                # ç²¾ç¡®åŒ¹é…ç±»å
                if short_name == class_name:
                    key_nodes.append(node_id)
                    matched = True
                    logging.info(f"ç²¾ç¡®åŒ¹é…æ‰¾åˆ°ç±» {class_name} => {name}")
                    break
            
            if not matched:
                if allow_fuzzy_matching:
                    # å¦‚æœæ‰¾ä¸åˆ°ç²¾ç¡®åŒ¹é…ï¼Œå°è¯•æ¨¡ç³ŠåŒ¹é…
                    logging.info(f"æ— æ³•æ‰¾åˆ°ç±» {class_name} çš„ç²¾ç¡®åŒ¹é…ï¼Œå°è¯•æ¨¡ç³ŠåŒ¹é…")
                    for name, node_id in name_to_node.items():
                        short_name = name.split('.')[-1]
                        if class_name in short_name:
                            key_nodes.append(node_id)
                            logging.info(f"ä½¿ç”¨æ¨¡ç³ŠåŒ¹é…æ‰¾åˆ° {class_name} => {name}")
                            matched = True
                            break
                
                if not matched:
                    not_found_classes.append(class_name)
                    logging.info(f"æ— æ³•æ‰¾åˆ°ç±» {class_name}")
        
        # å¦‚æœæœ‰æœªæ‰¾åˆ°çš„ç±»ï¼Œå¹¶ä¸”é…ç½®äº†è‡ªåŠ¨è¡¥å……ï¼Œåˆ™ä½¿ç”¨ä¸­å¿ƒæ€§è®¡ç®—æ¥è¡¥å……
        if not_found_classes and auto_complete_missing_classes:
            logging.info(f"ä½¿ç”¨ä¸­å¿ƒæ€§è®¡ç®—è¡¥å……æœªæ‰¾åˆ°çš„ {len(not_found_classes)} ä¸ªç±»")
            
            # è®¡ç®—åº¦ä¸­å¿ƒæ€§å’Œä»‹æ•°ä¸­å¿ƒæ€§
            degree_centrality = nx.degree_centrality(G)
            betweenness_centrality = nx.betweenness_centrality(G, weight='weight')
            combined_centrality = {node: degree_centrality[node] + betweenness_centrality[node] for node in G.nodes()}
            
            # æŒ‰ä¸­å¿ƒæ€§æ’åº
            sorted_nodes = sorted(combined_centrality.items(), key=lambda x: x[1], reverse=True)
            
            # é€‰æ‹©æœ€é‡è¦çš„èŠ‚ç‚¹ï¼Œä½†è¦æ’é™¤å·²ç»é€‰æ‹©çš„å…³é”®èŠ‚ç‚¹
            additional_nodes = []
            for node, centrality in sorted_nodes:
                if node not in key_nodes:
                    additional_nodes.append(node)
                    logging.info(f"è‡ªåŠ¨è¡¥å……æ·»åŠ èŠ‚ç‚¹ {node} => {node_mapping[node]}, ä¸­å¿ƒæ€§: {centrality:.4f}")
                    if len(additional_nodes) == len(not_found_classes):
                        break
            
            key_nodes.extend(additional_nodes)
        
        # å»é‡
        key_nodes = list(set(key_nodes))
        
        # è®¾ç½®å…³é”®ç±»æ ‡ç­¾
        for node in key_nodes:
            labels[node] = 1
            
        logging.info(f"ä½¿ç”¨é¢„å®šä¹‰çš„å…³é”®ç±»: {key_class_names}")
        logging.info(f"å®é™…æ‰¾åˆ°çš„å…³é”®ç±»æ•°é‡: {len(key_nodes)}")
        if not_found_classes:
            logging.info(f"æœªæ‰¾åˆ°çš„å…³é”®ç±»: {not_found_classes}")

    else:
        # è®¡ç®—åº¦ä¸­å¿ƒæ€§
        degree_centrality = nx.degree_centrality(G)
        # è®¡ç®—ä»‹æ•°ä¸­å¿ƒæ€§ï¼Œè€ƒè™‘è¾¹çš„æƒé‡
        betweenness_centrality = nx.betweenness_centrality(G, weight='weight')

        # ç»“åˆåº¦ä¸­å¿ƒæ€§å’Œä»‹æ•°ä¸­å¿ƒæ€§
        combined_centrality = {node: degree_centrality[node] + betweenness_centrality[node] for node in G.nodes()}

        # é€‰æ‹©å‰20%çš„èŠ‚ç‚¹ä½œä¸ºå…³é”®ç±»
        # num_key_nodes = max(1, int(key_class_percentage * len(G.nodes())))
        key_nodes = sorted(combined_centrality.items(), key=lambda x: x[1], reverse=True)[:num_key_nodes]
        key_nodes = [node for node, centrality in key_nodes]

        # è®¾ç½®å…³é”®ç±»æ ‡ç­¾
        for node in key_nodes:
            labels[node] = 1
            
        logging.info(f"ä½¿ç”¨ä¸­å¿ƒæ€§è®¡ç®—å¾—åˆ°çš„å…³é”®ç±»")
    
    # è¾“å‡ºå…³é”®ç±»çš„åç§°
    print("\nå…³é”®ç±»åˆ—è¡¨:")
    for node in key_nodes:
        if node_mapping:
            print(f"èŠ‚ç‚¹ID: {node}, ç±»å: {node_mapping[node]}")
        else:
            print(f"èŠ‚ç‚¹ID: {node}")
    
    return labels, key_nodes


# 1.2 ç½‘ç»œåµŒå…¥å­¦ä¹   å¾—åˆ°èŠ‚ç‚¹å‘é‡(embeddings)
def compute_node_embeddings(G, dimensions=16):
    """
    ä½¿ç”¨ Node2Vec ç”ŸæˆèŠ‚ç‚¹åµŒå…¥å‘é‡ã€‚

    :param G: NetworkX å›¾
    :param dimensions: åµŒå…¥ç»´åº¦
    :return: èŠ‚ç‚¹åµŒå…¥çš„ NumPy æ•°ç»„
    """
    for u, v, data in G.edges(data=True):
        if 'weight' not in data or data['weight'] <= 0:
            print(f"è­¦å‘Š: è¾¹({u},{v})æœ‰æ— æ•ˆæƒé‡ {data.get('weight', 'æ— ')}")
            data['weight'] = 1.0  # è®¾ç½®é»˜è®¤æƒé‡
    node2vec = Node2Vec(
        G,
        dimensions=dimensions,
        walk_length=30,  # å¢åŠ æ¸¸èµ°é•¿åº¦ä»¥æ•è·æ›´å¤šç»“æ„ä¿¡æ¯
        num_walks=100,  # å¢åŠ æ¸¸èµ°æ¬¡æ•°ä»¥æé«˜é‡‡æ ·è¦†ç›–ç‡
        workers=1,
        p=node2vec_p_param,
        q=node2vec_q_param,
        weight_key='weight',
        quiet=True
    )
    model = node2vec.fit(window=10, min_count=1, batch_words=4)  # å¢åŠ çª—å£å¤§å°
    
    # å°†æ¯ä¸ªèŠ‚ç‚¹çš„åµŒå…¥å‘é‡å­˜å‚¨ä¸º NumPy æ•°ç»„
    embeddings = np.array([model.wv[str(node)] for node in G.nodes()])
    return embeddings


# è®¡ç®—å¤šç§ä¸­å¿ƒæ€§æŒ‡æ ‡    å¾—åˆ°Gå›¾æ‰€æœ‰èŠ‚ç‚¹çš„ä¸­å¿ƒæ€§ç‰¹å¾(centrality_features)-å½¢çŠ¶ï¼šï¼ˆèŠ‚ç‚¹æ•°, 17ï¼‰çš„numpyæ•°ç»„
def compute_centrality_features(G):
    """
    è®¡ç®—å›¾ä¸­æ¯ä¸ªèŠ‚ç‚¹çš„å¤šç§ä¸­å¿ƒæ€§æŒ‡æ ‡ä½œä¸ºç‰¹å¾ã€‚
    å¢å¼ºç‰ˆæœ¬ï¼ŒåŠ å…¥æ›´å¤šä¸­å¿ƒæ€§æŒ‡æ ‡å’Œç‰¹å¾å½’ä¸€åŒ–ã€‚
    
    :param G: NetworkX å›¾
    :return: èŠ‚ç‚¹ä¸­å¿ƒæ€§ç‰¹å¾çš„ NumPy æ•°ç»„
    """
    logging.info("è®¡ç®—å¢å¼ºçš„èŠ‚ç‚¹ä¸­å¿ƒæ€§ç‰¹å¾...")
    
    # åŸºæœ¬ä¸­å¿ƒæ€§æŒ‡æ ‡
    # åº¦ä¸­å¿ƒæ€§  degree_centrality
    degree_centrality = nx.degree_centrality(G) 
    # ä»‹æ•°ä¸­å¿ƒæ€§    betweenness_centrality
    betweenness_centrality = nx.betweenness_centrality(G, weight='weight')
    # å°è¯•ä½¿ç”¨åŠ æƒæ¥è¿‘ä¸­å¿ƒæ€§    closeness_centrality
    try:
        closeness_centrality = nx.closeness_centrality(G, distance='weight')
    except:
        closeness_centrality = nx.closeness_centrality(G)
    
    # ç‰¹å¾å‘é‡ä¸­å¿ƒæ€§    eigenvector_centrality
    try:
        eigenvector_centrality = nx.eigenvector_centrality(G, weight='weight', max_iter=1000)
    except:
        try:
            eigenvector_centrality = nx.eigenvector_centrality(G, max_iter=1000)
        except:
            eigenvector_centrality = nx.pagerank(G, weight='weight')
    
    # é¡µé¢æ’å - ä½¿ç”¨æ›´å¤šè¿­ä»£æ¬¡æ•°æé«˜å‡†ç¡®æ€§
    pagerank = nx.pagerank(G, weight='weight', max_iter=200, tol=1e-6)
    # è®¡ç®—åŠ æƒPageRankå˜ä½“      pagerank_variants
    alpha_values = [0.7, 0.85, 0.95]
    pagerank_variants = []
    for alpha in alpha_values:
        try:
            pr = nx.pagerank(G, weight='weight', alpha=alpha)
            pagerank_variants.append(pr)
        except:
            # å¦‚æœå¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤pagerank
            pagerank_variants.append(pagerank)
    
    # å±€éƒ¨èšç±»ç³»æ•°  clustering
    try:
        clustering = nx.clustering(G, weight='weight')
    except:
        clustering = nx.clustering(G)
    
    # è®¡ç®—å…¥åº¦å’Œå‡ºåº¦
    in_degree = {node: G.in_degree(node, weight='weight') for node in G.nodes()}
    out_degree = {node: G.out_degree(node, weight='weight') for node in G.nodes()}
    
    # è®¡ç®—æ ¸å¿ƒéƒ¨åˆ† (k-core) æŒ‡æ ‡ - è¡¨ç¤ºèŠ‚ç‚¹åœ¨ç½‘ç»œä¸­çš„"æ ¸å¿ƒæ€§"
    try:
        k_core_numbers = nx.core_number(G.to_undirected())
    except:
        # å¦‚æœå¤±è´¥ï¼Œä½¿ç”¨åŸºäºåº¦çš„è¿‘ä¼¼
        k_core_numbers = {node: (degree_centrality[node] * 10) for node in G.nodes()}
    
    # è®¡ç®—HITSç®—æ³•çš„hubå’Œauthorityåˆ†æ•°
    try:
        hits_scores = nx.hits(G, max_iter=200)
        hubs, authorities = hits_scores[0], hits_scores[1]
    except:
        # å¦‚æœHITSå¤±è´¥ï¼Œä½¿ç”¨åº¦ä¸­å¿ƒæ€§ä½œä¸ºè¿‘ä¼¼
        hubs = degree_centrality
        authorities = degree_centrality
    
    # æ ‡å‡†åŒ–ä¸­å¿ƒæ€§æŒ‡æ ‡
    if standardize_features:
        logging.info("æ ‡å‡†åŒ–ä¸­å¿ƒæ€§ç‰¹å¾...")
        scaler = StandardScaler()
        centrality_features = []    # å½¢çŠ¶ä¸º(èŠ‚ç‚¹æ•°, 17)çš„NunPyæ•°å€¼
        
        for node in G.nodes():      # æ¯ä¸ªèŠ‚ç‚¹å…·æœ‰ä»¥ä¸‹17ä¸ªç‰¹å¾
            node_features = [
                degree_centrality[node],
                betweenness_centrality[node],
                closeness_centrality[node],
                eigenvector_centrality[node],
                pagerank[node],
                clustering.get(node, 0),
                in_degree[node],
                out_degree[node],
                # æ·»åŠ æ–°ç‰¹å¾
                k_core_numbers.get(node, 0),
                hubs.get(node, 0),
                authorities.get(node, 0),
                # PageRankå˜ä½“
                pagerank_variants[0].get(node, 0),
                pagerank_variants[1].get(node, 0),
                pagerank_variants[2].get(node, 0),
                # æ´¾ç”Ÿç‰¹å¾
                in_degree[node] / (out_degree[node] + 1e-6),  # å…¥åº¦/å‡ºåº¦æ¯”ç‡
                in_degree[node] * out_degree[node],           # å…¥åº¦å’Œå‡ºåº¦çš„ä¹˜ç§¯
                in_degree[node] + out_degree[node]            # å…¥åº¦å’Œå‡ºåº¦çš„å’Œ
            ]
            centrality_features.append(node_features)
        
        # æ ‡å‡†åŒ–ç‰¹å¾
        centrality_features = np.array(centrality_features)
        centrality_features = scaler.fit_transform(centrality_features)
    else:
        # ä¸æ ‡å‡†åŒ–ï¼Œç›´æ¥æ„å»ºç‰¹å¾æ•°ç»„
        centrality_features = np.array([
            [
                degree_centrality[node],        # 0.åº¦ä¸­å¿ƒæ€§
                betweenness_centrality[node],   # 1.ä»‹æ•°ä¸­å¿ƒæ€§
                closeness_centrality[node],     # 2.æ¥è¿‘ä¸­å¿ƒæ€§
                eigenvector_centrality[node],   # 3.ç‰¹å¾å‘é‡ä¸­å¿ƒæ€§
                pagerank[node],                 # 4.é¡µé¢æ’å
                clustering.get(node, 0),        # 5.å±€éƒ¨èšç±»ç³»æ•°
                in_degree[node],                # 6.åŠ æƒå…¥åº¦
                out_degree[node],               # 7.åŠ æƒå‡ºåº¦
                # æ·»åŠ æ–°ç‰¹å¾
                k_core_numbers.get(node, 0),    # 8.k-core æ•°é‡
                hubs.get(node, 0),              # 9.HITSç®—æ³•çš„hubåˆ†æ•°
                authorities.get(node, 0),       # 10.HITSç®—æ³•çš„authorityåˆ†æ•°
                # PageRankå˜ä½“
                pagerank_variants[0].get(node, 0),  # 11.PageRankå˜ä½“1
                pagerank_variants[1].get(node, 0),  # 12.PageRankå˜ä½“2
                pagerank_variants[2].get(node, 0),  # 13.PageRankå˜ä½“3
                # æ´¾ç”Ÿç‰¹å¾
                in_degree[node] / (out_degree[node] + 1e-6),  # 14.å…¥åº¦/å‡ºåº¦æ¯”ç‡
                in_degree[node] * out_degree[node],           # 15.å…¥åº¦å’Œå‡ºåº¦çš„ä¹˜ç§¯
                in_degree[node] + out_degree[node]            # 16.å…¥åº¦å’Œå‡ºåº¦çš„å’Œ
            ] for node in G.nodes()
        ])
    
    # åº”ç”¨ç‰¹å¾é‡è¦æ€§æƒé‡
    if transfer_learning_settings['feature_weighting']:
        # ä¸ºä¸åŒç±»å‹çš„ç‰¹å¾è®¾ç½®æƒé‡
        weights = np.ones(centrality_features.shape[1])
        
        # ç»“æ„ç‰¹å¾æƒé‡ (0-7)
        weights[0:8] = 1.5
        
        # æ‹“æ‰‘ç‰¹å¾æƒé‡ (8-10)
        weights[8:11] = 1.2
        
        # PageRankå˜ä½“ (11-13)
        weights[11:14] = 1.3
        
        # æ´¾ç”Ÿç‰¹å¾ (14-16)
        weights[14:17] = 1.1
        
        # åº”ç”¨æƒé‡
        weighted_features = centrality_features * weights
        
        logging.info(f"åº”ç”¨ç‰¹å¾é‡è¦æ€§æƒé‡åçš„ä¸­å¿ƒæ€§ç‰¹å¾å½¢çŠ¶: {weighted_features.shape}")
        return weighted_features
    
    logging.info(f"å¢å¼ºçš„ä¸­å¿ƒæ€§ç‰¹å¾è®¡ç®—å®Œæˆï¼Œå½¢çŠ¶: {centrality_features.shape}")
    return centrality_features


# ç»„åˆèŠ‚ç‚¹åµŒå…¥å’Œä¸­å¿ƒæ€§ç‰¹å¾  embedingæ˜¯å¦å’Œä¸Šé¢çš„ä¸­å¿ƒæ€§ç‰¹å¾ç»„åˆï¼ˆembeddingsï¼‰
def combine_features(embeddings, centrality_features=None):
    """
    ç»„åˆèŠ‚ç‚¹åµŒå…¥å’Œä¸­å¿ƒæ€§ç‰¹å¾ã€‚
    
    :param embeddings: èŠ‚ç‚¹åµŒå…¥å‘é‡
    :param centrality_features: ä¸­å¿ƒæ€§ç‰¹å¾
    :return: ç»„åˆåçš„ç‰¹å¾å‘é‡
    """
    if centrality_features is not None and use_multiple_centrality:
        # ç»„åˆç‰¹å¾
        combined_features = np.hstack((embeddings, centrality_features))
        
        # æ ‡å‡†åŒ–ç»„åˆç‰¹å¾
        if standardize_features:
            scaler = StandardScaler()
            combined_features = scaler.fit_transform(combined_features)
        
        return combined_features
    else:
        # åªä½¿ç”¨åµŒå…¥ç‰¹å¾
        if standardize_features:
            scaler = StandardScaler()
            embeddings = scaler.fit_transform(embeddings)
        
        return embeddings



# -----------------------------------
# è¾…åŠ©å‡½æ•°ï¼šæ•°æ®é›†åˆ’åˆ†ï¼ˆæœªè°ƒç”¨ï¼‰
# -----------------------------------
def split_data(labels, key_nodes, num_splits=10, test_size=1, random_seed=42):
    """
    å°†æ•°æ®é›†åˆ’åˆ†ä¸ºè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼Œç¡®ä¿å…³é”®ç±»åœ¨ä¸¤è€…ä¸­å‡åŒ€åˆ†å¸ƒã€‚

    :param labels: å…¨éƒ¨èŠ‚ç‚¹çš„æ ‡ç­¾æ•°ç»„
    :param key_nodes: å…³é”®ç±»èŠ‚ç‚¹åˆ—è¡¨
    :param num_splits: åˆ’åˆ†æ¬¡æ•°
    :param test_size: æµ‹è¯•é›†æ¯”ä¾‹ï¼ˆå…³é”®ç±»ï¼‰
    :param random_seed: éšæœºç§å­
    :return: åˆ’åˆ†åçš„è®­ç»ƒé›†å’Œæµ‹è¯•é›†æ ‡ç­¾åˆ—è¡¨
    """
    splits = []
    for i in range(num_splits):
        # è®¾ç½®ä¸åŒçš„éšæœºç§å­ä»¥ä¿è¯ä¸åŒçš„åˆ’åˆ†
        seed = random_seed + i
        # åˆ’åˆ†å…³é”®ç±»
        train_keys, test_keys = train_test_split(key_nodes, test_size=test_size, random_state=seed)

        # åˆ›å»ºè®­ç»ƒå’Œæµ‹è¯•æ ‡ç­¾
        train_labels = np.zeros_like(labels)
        test_labels = np.zeros_like(labels)

        # è®­ç»ƒé›†ä¸­çš„å…³é”®ç±»
        for node in train_keys:
            train_labels[node] = 1
        # æµ‹è¯•é›†ä¸­çš„å…³é”®ç±»
        for node in test_keys:
            test_labels[node] = 1

        splits.append((train_labels, test_labels))

    return splits

# æ¨¡å‹ä¿å­˜ç›®å½•
model_dir = "/home/ps/jy_exp/output/GNN_res/models"
# è®­ç»ƒå¥½çš„é€šç”¨æ¨¡å‹è·¯å¾„
general_model_path = os.path.join(model_dir, "general_model.pth")

def save_net_file(G, file_path):
    """ä¿å­˜NetworkXå›¾ä¸º.netæ ¼å¼"""
    with open(file_path, 'w', encoding='utf-8') as f:
        # å†™å…¥èŠ‚ç‚¹
        f.write(f"*Vertices {G.number_of_nodes()}\n")
        for node in sorted(G.nodes()):
            name = G.nodes[node].get('name', f'Node{node}')
            f.write(f'{node} "{name}"\n')
        
        # å†™å…¥è¾¹
        f.write("*Arcs\n")
        for u, v, data in G.edges(data=True):
            weight = data.get('weight', 1.0)
            f.write(f"{u} {v} {weight}\n")

def parse_net_file(file_path):
    """è§£æ.netæ–‡ä»¶ä¸ºNetworkXå›¾"""
    G = nx.DiGraph()
    node_mapping = {}
    
    with open(file_path, 'r', encoding='utf-8') as f:
        lines = f.readlines()
    
    vertices_section = False
    arcs_section = False
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
            
        if line.startswith('*Vertices'):
            vertices_section = True
            arcs_section = False
            continue
            
        if line.startswith('*Arcs'):
            vertices_section = False
            arcs_section = True
            continue
            
        if vertices_section:
            # è§£æèŠ‚ç‚¹è¡Œ: "1 \"ClassName\""
            match = re.match(r'(\d+)\s+"(.+)"', line)
            if match:
                node_id = int(match.group(1))
                node_name = match.group(2)
                G.add_node(node_id, name=node_name)
                node_mapping[node_id] = node_name
                
        elif arcs_section:
            # è§£æè¾¹è¡Œ: "source target weight"
            parts = line.split()
            if len(parts) >= 2:
                source = int(parts[0])
                target = int(parts[1])
                weight = float(parts[2]) if len(parts) >= 3 else 1.0
                G.add_edge(source, target, weight=weight)
    
    return G, node_mapping

def create_combined_dataset():
    """åˆ›å»ºç»„åˆæ•°æ®é›†"""
    # 1. åˆ›å»ºæ–°æ•°æ®é›†ç›®å½•
    new_dataset_path = Path(BASE_PATH) / f"{NEW_DATASET}_{net_type}"
    new_dataset_path.mkdir(parents=True, exist_ok=True)
    net_file = new_dataset_path / f"combined_{NEW_DATASET}_GN.net"
    
    # 2. åˆå§‹åŒ–æ–°å›¾çš„å®¹å™¨
    combined_graph = nx.DiGraph()
    node_id_map = {}  # å­˜å‚¨(åŸå§‹æ•°æ®é›†, åŸå§‹ID) -> æ–°ID
    next_node_id = 1
    
    # 3. å¤„ç†æ¯ä¸ªæ•°æ®é›†
    for dataset in DATASETS:
        # 3.1 è¯»å–åŸå§‹æ•°æ®é›†
        dataset_path = Path(BASE_PATH) / f"{dataset}_{net_type}"
        source_file = dataset_path / f"combined_{dataset}_GN.net"
        
        if not source_file.exists():
            print(f"âš ï¸ è·³è¿‡ç¼ºå¤±çš„æ•°æ®é›†: {source_file}")
            continue
        # 3.2 è§£æ.netæ–‡ä»¶
        graph, name_map = parse_net_file(source_file)
        
        # 3.3 éšæœºé‡‡æ ·èŠ‚ç‚¹
        all_nodes = list(graph.nodes())
        sampled_nodes = random.sample(all_nodes, int(len(all_nodes) * SAMPLE_RATIO))
        
        # 3.4 æ·»åŠ é‡‡æ ·èŠ‚ç‚¹åˆ°æ–°å›¾
        for orig_node in sampled_nodes:
            # åˆ›å»ºæ–°èŠ‚ç‚¹ID
            new_node_id = next_node_id
            next_node_id += 1
            
            # ä¿å­˜æ˜ å°„å…³ç³»
            node_id_map[(dataset, orig_node)] = new_node_id
            
            # è·å–ç±»åï¼ˆæ·»åŠ æ•°æ®é›†å‰ç¼€é¿å…å†²çªï¼‰
            class_name = f"{dataset}_{name_map.get(orig_node, f'Node{orig_node}')}"
            
            # æ·»åŠ åˆ°æ–°å›¾
            combined_graph.add_node(new_node_id, name=class_name)
        
        # 3.5 æ·»åŠ ç›¸å…³è¾¹
        for orig_node in sampled_nodes:
            # è·å–åŸå§‹èŠ‚ç‚¹çš„æ‰€æœ‰å‡ºè¾¹
            for neighbor in graph.successors(orig_node):
                # åªæ·»åŠ ä¸¤ä¸ªç«¯ç‚¹éƒ½è¢«é‡‡æ ·çš„è¾¹
                if neighbor in sampled_nodes:
                    weight = graph[orig_node][neighbor].get('weight', 1.0)
                    src = node_id_map[(dataset, orig_node)]
                    dst = node_id_map[(dataset, neighbor)]
                    combined_graph.add_edge(src, dst, weight=weight)
    
    # 4. ä¿å­˜æ–°æ•°æ®é›†
    save_net_file(combined_graph, net_file)
    print(f"âœ… åˆ›å»ºç»„åˆæ•°æ®é›†å®Œæˆ! å…± {combined_graph.number_of_nodes()} ä¸ªèŠ‚ç‚¹")
    print(f"ğŸ“ è·¯å¾„: {net_file}")
    
    return net_file

def compute_centrality_scores(G):
    """
    è®¡ç®—8ç§ä¸­å¿ƒæ€§æŒ‡æ ‡å¾—åˆ†
    :param G: NetworkX å›¾
    :return: å­—å…¸ï¼Œé”®ä¸ºæ–¹æ³•åï¼Œå€¼ä¸º{èŠ‚ç‚¹: å¾—åˆ†}çš„å­—å…¸
    """
    centrality_scores = {}
    
    # 1. Degree (åº¦ä¸­å¿ƒæ€§)
    centrality_scores['Degree'] = nx.degree_centrality(G)
    
    # 2. In-Degree (å…¥åº¦ä¸­å¿ƒæ€§)
    in_degree = dict(G.in_degree(weight='weight'))
    max_in_degree = max(in_degree.values()) if in_degree else 1
    centrality_scores['In-Degree'] = {node: deg/max_in_degree for node, deg in in_degree.items()}
    
    # 3. PageRank
    centrality_scores['PageRank'] = nx.pagerank(G, weight='weight')
    
    # 4. Betweenness (ä»‹æ•°ä¸­å¿ƒæ€§)
    centrality_scores['Betweenness'] = nx.betweenness_centrality(G, weight='weight')
    
    # 5. HITS (ä½¿ç”¨authorityåˆ†æ•°)
    try:
        _, authorities = nx.hits(G, max_iter=200)
        centrality_scores['HITS'] = authorities
    except:
        # å¦‚æœHITSå¤±è´¥ï¼Œä½¿ç”¨åº¦ä¸­å¿ƒæ€§ä½œä¸ºåå¤‡
        centrality_scores['HITS'] = nx.degree_centrality(G)
    
    # 6. Cores (k-core)
    try:
        # è½¬ä¸ºæ— å‘å›¾è®¡ç®—k-core
        undirected_G = G.to_undirected()
        core_numbers = nx.core_number(undirected_G)
        max_core = max(core_numbers.values()) if core_numbers else 1
        centrality_scores['Cores'] = {node: core/max_core for node, core in core_numbers.items()}
    except:
        centrality_scores['Cores'] = nx.degree_centrality(G)
    
    # 7. Weighted K-core (åŠ æƒæ ¸åˆ†è§£)
    try:
        # ä½¿ç”¨åŠ æƒåº¦è®¡ç®—
        weighted_degrees = dict(undirected_G.degree(weight='weight'))
        max_wdegree = max(weighted_degrees.values()) if weighted_degrees else 1
        centrality_scores['Weighted_Kcore'] = {node: deg/max_wdegree for node, deg in weighted_degrees.items()}
    except:
        centrality_scores['Weighted_Kcore'] = nx.degree_centrality(G)
    
    # 8. Minclass (æœ€å°ç±»ä¸­å¿ƒæ€§ - è‡ªå®šä¹‰æŒ‡æ ‡)
    # è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨èŠ‚ç‚¹åˆ°å›¾ä¸­æœ€ä¸é‡è¦çš„èŠ‚ç‚¹(åº¦æœ€ä½)çš„è·ç¦»çš„å€’æ•°
    min_degree_node = min(G.degree(), key=lambda x: x[1])[0]
    centrality_scores['Minclass'] = {}
    for node in G.nodes():
        try:
            # è®¡ç®—åˆ°æœ€å°åº¦èŠ‚ç‚¹çš„æœ€çŸ­è·¯å¾„é•¿åº¦
            path_length = nx.shortest_path_length(G, source=node, target=min_degree_node, weight='weight')
            centrality_scores['Minclass'][node] = 1 / (path_length + 1)  # åŠ 1é¿å…é™¤é›¶
        except:
            # å¦‚æœèŠ‚ç‚¹ä¸å¯è¾¾ï¼Œèµ‹äºˆä¸€ä¸ªä½åˆ†å€¼
            centrality_scores['Minclass'][node] = 0.01
    
    return centrality_scores

def identify_key_classes(G, threshold_count=4, top_percent=0.15):
    """
    ä½¿ç”¨8ç§æ–¹æ³•è¯†åˆ«å…³é”®ç±»
    :param G: NetworkX å›¾
    :param threshold_count: è®¤å®šä¸ºå…³é”®ç±»æ‰€éœ€çš„æœ€å°æŠ•ç¥¨æ¬¡æ•°
    :param top_percent: æ¯ç§æ–¹æ³•é€‰æ‹©çš„æ¯”ä¾‹
    :return: å…³é”®ç±»åˆ—è¡¨ï¼ŒæŠ•ç¥¨ç»Ÿè®¡
    """
    # 1. è®¡ç®—æ‰€æœ‰ä¸­å¿ƒæ€§å¾—åˆ†
    centrality_scores = compute_centrality_scores(G)
    
    # 2. ä¸ºæ¯ç§æ–¹æ³•é€‰æ‹©å‰15%çš„èŠ‚ç‚¹
    candidate_key_classes = {method: [] for method in centrality_scores}
    all_candidates = []
    
    for method, scores in centrality_scores.items():
        # æŒ‰å¾—åˆ†é™åºæ’åº
        sorted_nodes = sorted(scores.items(), key=lambda x: x[1], reverse=True)
        
        # è®¡ç®—è¦é€‰æ‹©çš„èŠ‚ç‚¹æ•°é‡
        num_nodes = len(sorted_nodes)
        num_select = max(1, int(num_nodes * top_percent))  # è‡³å°‘é€‰æ‹©1ä¸ªèŠ‚ç‚¹
        
        # é€‰æ‹©å‰top_percentçš„èŠ‚ç‚¹
        selected_nodes = [node for node, score in sorted_nodes[:num_select]]
        candidate_key_classes[method] = selected_nodes
        all_candidates.extend(selected_nodes)
    
    # 3. ç»Ÿè®¡æ¯ä¸ªèŠ‚ç‚¹è¢«é€‰ä¸­çš„æ¬¡æ•°
    candidate_counter = Counter(all_candidates)
    
    # 4. ç¡®å®šæœ€ç»ˆå…³é”®ç±»(è¢«è‡³å°‘threshold_countç§æ–¹æ³•é€‰ä¸­)
    key_classes = [node for node, count in candidate_counter.items() if count > threshold_count]
    
    return key_classes, candidate_key_classes, candidate_counter

def print_key_class_results(G, key_classes, candidate_key_classes, candidate_counter, node_mapping=None, log_file="log.txt"):
    """
    æ‰“å°å…³é”®ç±»è¯†åˆ«ç»“æœï¼Œå¹¶å°†å…³é”®ç±»IDå’Œç±»åå†™å…¥æ—¥å¿—æ–‡ä»¶
    :param G: NetworkXå›¾
    :param key_classes: å…³é”®ç±»èŠ‚ç‚¹IDåˆ—è¡¨
    :param candidate_key_classes: æ¯ç§æ–¹æ³•çš„å€™é€‰ç±»
    :param candidate_counter: èŠ‚ç‚¹è¢«é€‰ä¸­æ¬¡æ•°ç»Ÿè®¡
    :param node_mapping: èŠ‚ç‚¹IDåˆ°ç±»åçš„æ˜ å°„å­—å…¸
    :param log_file: æ—¥å¿—æ–‡ä»¶è·¯å¾„
    """
    # æ‰“å¼€æ—¥å¿—æ–‡ä»¶
    with open(log_file, "w") as log:
        # å†™å…¥æ—¥å¿—æ ‡é¢˜
        log.write("=" * 80 + "\n")
        log.write("å…³é”®ç±»è¯†åˆ«ç»“æœ\n")
        log.write("=" * 80 + "\n\n")
        
        # 1. æ˜¾ç¤ºæ¯ç§æ–¹æ³•é€‰æ‹©çš„å€™é€‰ç±»
        log.write("æ¯ç§æ–¹æ³•é€‰æ‹©çš„å‰15%å€™é€‰ç±»:\n")
        log.write("-" * 60 + "\n")
        for method, candidates in candidate_key_classes.items():
            log.write(f"{method:>15}: {len(candidates)}ä¸ªèŠ‚ç‚¹\n")
        log.write("\n")
        
        # 2. æ˜¾ç¤ºæŠ•ç¥¨ç»Ÿè®¡
        log.write("èŠ‚ç‚¹æŠ•ç¥¨ç»Ÿè®¡:\n")
        log.write("-" * 60 + "\n")
        log.write("èŠ‚ç‚¹ID\tç±»å\tè¢«é€‰ä¸­æ¬¡æ•°\tæ–¹æ³•åˆ—è¡¨\n")
        log.write("-" * 60 + "\n")
        
        # æŒ‰è¢«é€‰ä¸­æ¬¡æ•°é™åºæ’åº
        sorted_counter = candidate_counter.most_common()
        
        for node, count in sorted_counter:
            # è·å–ç±»å
            class_name = node_mapping[node] if node_mapping and node in node_mapping else f"Node-{node}"
            
            # è·å–é€‰ä¸­è¯¥èŠ‚ç‚¹çš„æ–¹æ³•
            methods = [method for method, candidates in candidate_key_classes.items() if node in candidates]
            
            log.write(f"{node}\t{class_name}\t{count}\t\t{', '.join(methods)}\n")
        log.write("\n")
        
        # 3. æ˜¾ç¤ºæœ€ç»ˆå…³é”®ç±»
        log.write(f"æœ€ç»ˆå…³é”®ç±»(è¢«4ç§ä»¥ä¸Šæ–¹æ³•é€‰ä¸­):\n")
        log.write("-" * 60 + "\n")
        for node in key_classes:
            # è·å–ç±»å
            class_name = node_mapping[node] if node_mapping and node in node_mapping else f"Node-{node}"
            
            # è·å–èŠ‚ç‚¹åº¦ä¿¡æ¯
            in_degree = G.in_degree(node, weight='weight')
            out_degree = G.out_degree(node, weight='weight')
            total_degree = in_degree + out_degree
            
            # è·å–èŠ‚ç‚¹è¢«å“ªäº›æ–¹æ³•é€‰ä¸­
            methods = [method for method, candidates in candidate_key_classes.items() if node in candidates]
            
            log.write(f"èŠ‚ç‚¹ {node}: {class_name}\n")
            log.write(f"  åº¦={total_degree} (å…¥åº¦={in_degree}, å‡ºåº¦={out_degree})\n")
            log.write(f"  è¢«{len(methods)}ç§æ–¹æ³•é€‰ä¸­: {', '.join(methods)}\n")
            log.write("\n")
        
        # 4. ç»Ÿè®¡ä¿¡æ¯
        num_nodes = len(G.nodes())
        num_key_classes = len(key_classes)
        log.write("\nç»Ÿè®¡æ‘˜è¦:\n")
        log.write("-" * 60 + "\n")
        log.write(f"æ€»èŠ‚ç‚¹æ•°: {num_nodes}\n")
        log.write(f"å€™é€‰å…³é”®ç±»æ€»æ•°: {len(candidate_counter)}\n")
        log.write(f"æœ€ç»ˆå…³é”®ç±»æ•°é‡: {num_key_classes} ({num_key_classes/num_nodes:.2%})\n")
        
        # 5. åªåŒ…å«å…³é”®ç±»IDå’Œç±»åçš„ç®€æ´åˆ—è¡¨
        log.write("\nå…³é”®ç±»åˆ—è¡¨ (ID å’Œ ç±»å):\n")
        log.write("-" * 60 + "\n")
        for node in key_classes:
            class_name = node_mapping[node] if node_mapping and node in node_mapping else f"Node-{node}"
            log.write(f"{node} - {class_name}\n")
    
    # åŒæ—¶æ‰“å°ä¸€äº›å…³é”®ä¿¡æ¯åˆ°æ§åˆ¶å°
    print(f"ç»“æœå·²å†™å…¥æ—¥å¿—æ–‡ä»¶: {log_file}")
    print(f"è¯†åˆ«å‡º {len(key_classes)} ä¸ªå…³é”®ç±»")
    print(f"è¯¦ç»†ç»“æœè¯·æŸ¥çœ‹: {log_file}")
    
    return key_classes

# -----------------------------------
# 2. æ¨¡å‹æ„å»º
# -----------------------------------

# 2.1 èŠ‚ç‚¹è¯„åˆ†ç½‘ç»œï¼ˆScoringNetï¼‰
class ScoringNet(nn.Module):
    """
    æ”¹è¿›çš„å…¨è¿æ¥ç¥ç»ç½‘ç»œï¼Œç”¨äºå°†èŠ‚ç‚¹åµŒå…¥å‘é‡æ˜ å°„ä¸ºåˆå§‹åˆ†å€¼ã€‚
    åŒ…å«æ›´å¤šå±‚ã€Dropoutå’ŒBatchNormä»¥å¢å¼ºæ³›åŒ–èƒ½åŠ›ã€‚
    """

    def __init__(self, input_dim, hidden_dims=[64, 32], dropout_rate=0.3, use_batch_norm=True):
        super(ScoringNet, self).__init__()
        
        layers = []
        prev_dim = input_dim
        
        # æ„å»ºå¤šå±‚ç¥ç»ç½‘ç»œ
        for i, hidden_dim in enumerate(hidden_dims):
            # çº¿æ€§å±‚
            layers.append(nn.Linear(prev_dim, hidden_dim))
            # æ‰¹å½’ä¸€åŒ–å±‚
            if use_batch_norm:
                layers.append(nn.BatchNorm1d(hidden_dim))
            # æ¿€æ´»å‡½æ•°
            layers.append(nn.LeakyReLU(0.2))
            # Dropoutå±‚
            layers.append(nn.Dropout(dropout_rate))
            prev_dim = hidden_dim
        
        # æœ€åçš„è¾“å‡ºå±‚
        layers.append(nn.Linear(prev_dim, 1))
        self.layers = nn.Sequential(*layers)

    def forward(self, x):
        return self.layers(x)


# 2.2 é‚»åŸŸèŠ‚ç‚¹åˆ†å€¼èšåˆ & 2.3 ä¸­å¿ƒæ€§è°ƒæ•´
# æ¨¡å‹
class GKCIModel(nn.Module):
    """
    æ”¹è¿›çš„GKCIæ¨¡å‹ï¼ŒåŒ…å«å¤šå±‚é‚»å±…åˆ†å€¼èšåˆå’Œä¸­å¿ƒæ€§è°ƒæ•´ã€‚
    å¢åŠ äº†æ­£åˆ™åŒ–æŠ€æœ¯å’Œæ›´çµæ´»çš„æ¶æ„ï¼Œæé«˜æ³›åŒ–èƒ½åŠ›ã€‚
    """

    def __init__(self, G, embedding_dim, hidden_dims=[64, 32], num_layers=2, 
                 num_sampled_neighbors=None, dropout_rate=0.3, use_batch_norm=True):
        super(GKCIModel, self).__init__()
        self.G = G
        self.nodes = list(G.nodes())
        self.node_to_idx = {node: idx for idx, node in enumerate(self.nodes)}
        self.idx_to_node = {idx: node for idx, node in enumerate(self.nodes)}
        self.num_nodes = len(self.nodes)
        self.num_layers = num_layers
        self.num_sampled_neighbors = num_sampled_neighbors
        self.dropout_rate = dropout_rate
        
        # 2.1 èŠ‚ç‚¹è¯„åˆ†ç½‘ç»œ - ä½¿ç”¨æ”¹è¿›çš„ScoringNet
        self.scoring_net = ScoringNet(
            embedding_dim, 
            hidden_dims=hidden_dims, 
            dropout_rate=dropout_rate, 
            use_batch_norm=use_batch_norm
        )
        
        # region ä¸­å¿ƒæ€§è°ƒæ•´ä¸­çš„å‚æ•°ï¼ˆbetaã€gamma)
        if ramdom_gama_beta:
            # ä½¿ç”¨æ›´ç¨³å®šçš„åˆå§‹åŒ–åˆ†å¸ƒ
            self.gamma = nn.Parameter(torch.randn(1) * 0.1 + 3.0)  # å‡å€¼ä¸º3.0ï¼Œæ ‡å‡†å·®ä¸º0.1
            self.beta = nn.Parameter(torch.randn(1) * 0.1 - 7.0)   # å‡å€¼ä¸º-7.0ï¼Œæ ‡å‡†å·®ä¸º0.1
        else:
            # ä½¿ç”¨ä¹‹å‰è®­ç»ƒå¥½çš„è¾ƒä¼˜å‚æ•°ï¼Œä½†å¢åŠ å¯è°ƒæ•´æ€§
            self.gamma = nn.Parameter(torch.tensor(GKCI_gama))
            self.beta = nn.Parameter(torch.tensor(GKCI_beta))
        logging.info(f"gamma: {self.gamma.item():.4f}, beta: {self.beta.item():.4f}")
        # endregion

        # æ–°å¢ï¼šè‡ªé€‚åº”ä¸­å¿ƒæ€§ç¼©æ”¾æœºåˆ¶ - æé«˜æ¨¡å‹åœ¨ä¸åŒå›¾ç»“æ„ä¸Šçš„é€‚åº”æ€§
        self.centrality_scaling = nn.Parameter(torch.tensor(1.0))
        # é¢„å¤„ç†é‚»æ¥ä¿¡æ¯
        self.preprocess_adjacency()
        # ç‰¹å¾æ··åˆæƒé‡ - ä½¿ç”¨Sigmoidæ¥ä¿è¯èŒƒå›´åœ¨0-1ä¹‹é—´
        self.mixing_weight_raw = nn.Parameter(torch.tensor(0.0))  # åˆå§‹åŒ–ä¸º0ï¼Œç»è¿‡sigmoidåä¸º0.5   
        # æ–°å¢ï¼šå™ªå£°æŠ‘åˆ¶é˜ˆå€¼
        self.noise_threshold = nn.Parameter(torch.tensor(0.05))

        # å¤šå±‚æ›´æ–°å‡½æ•°ï¼Œæ¯å±‚æœ‰ç‹¬ç«‹çš„å…¨è¿æ¥å±‚ï¼Œå¢åŠ äº†Dropoutå’ŒBatchNorm
        self.update_fc_layers = nn.ModuleList()
        for _ in range(num_layers):
            layer = []
            # çº¿æ€§å±‚ - å¢åŠ å®½åº¦ä»¥æé«˜è¡¨è¾¾èƒ½åŠ›
            layer.append(nn.Linear(3, 24))
            # æ‰¹å½’ä¸€åŒ–
            if use_batch_norm:
                layer.append(nn.BatchNorm1d(24))
            # ä½¿ç”¨LeakyReLUä»£æ›¿ReLUæé«˜æ¢¯åº¦æµåŠ¨æ€§
            layer.append(nn.LeakyReLU(0.1))
            # Dropout
            layer.append(nn.Dropout(dropout_rate))
            # éšè—å±‚
            layer.append(nn.Linear(24, 12))
            layer.append(nn.LeakyReLU(0.1))
            layer.append(nn.Dropout(dropout_rate * 0.5))  # å‡å°‘åå±‚çš„Dropoutä»¥ä¿ç•™æ›´å¤šä¿¡æ¯
            # è¾“å‡ºå±‚
            layer.append(nn.Linear(12, 1))
            layer.append(nn.Sigmoid())
            
            self.update_fc_layers.append(nn.Sequential(*layer))

    # å‰ç»§é‚»å±…å’Œåç»§é‚»å±…çš„ç´¢å¼•åŠå¯¹åº”çš„æƒé‡
    def preprocess_adjacency(self):
        self.pred_indices = []
        self.pred_weights = []
        self.succ_indices = []
        self.succ_weights = []

        for node in self.nodes:
            # å‰ç»§é‚»å±…
            preds = list(self.G.predecessors(node))
            pred_idx = [self.node_to_idx[n] for n in preds]
            pred_w = [self.G[n][node]['weight'] for n in preds]
            self.pred_indices.append(pred_idx)
            self.pred_weights.append(pred_w)

            # åç»§é‚»å±…
            succs = list(self.G.successors(node))
            succ_idx = [self.node_to_idx[n] for n in succs]
            succ_w = [self.G[node][n]['weight'] for n in succs]
            self.succ_indices.append(succ_idx)
            self.succ_weights.append(succ_w)


    def forward(self, embeddings, training=True):
        device = embeddings.device
        # åˆå§‹è¯„åˆ†
        layer_scores = self.scoring_net(embeddings).view(-1)  # [num_nodes]
        
        # ä¿å­˜åŸå§‹èŠ‚ç‚¹åˆ†æ•°ä»¥ä¾›åç»­ä½¿ç”¨
        original_scores = layer_scores.clone()

        for layer in range(self.num_layers):
            aggregated_scores_in = torch.zeros(self.num_nodes, device=device)
            aggregated_scores_out = torch.zeros(self.num_nodes, device=device)

            # region æ¯ä¸ªèŠ‚ç‚¹å‰ç»§å’Œåç»§é‚»å±…çš„åˆ†æ•°èšåˆï¼ˆaggregated_scores_in, aggregated_scores_out)
            for idx in range(self.num_nodes):
                # å‰ç»§é‚»å±…
                pred_idx = self.pred_indices[idx]
                pred_w = self.pred_weights[idx]

                if len(pred_idx) > 0:
                    # å¦‚æœéœ€è¦é‡‡æ ·é‚»å±…
                    if training and self.num_sampled_neighbors and len(pred_idx) > self.num_sampled_neighbors:
                        sampled = self.num_sampled_neighbors
                        # éšæœºé‡‡æ ·é‚»å±…
                        sampled_indices = random.sample(range(len(pred_idx)), sampled)
                        pred_idx_sample = [pred_idx[i] for i in sampled_indices]
                        pred_w_sample = [pred_w[i] for i in sampled_indices]
                    else:
                        pred_idx_sample = pred_idx
                        pred_w_sample = pred_w
                    
                    pred_idx_tensor = torch.tensor(pred_idx_sample, dtype=torch.long, device=device)
                    pred_w_tensor = torch.tensor(pred_w_sample, dtype=torch.float32, device=device)
                    
                    # åŠ æƒèšåˆå‰ç»§é‚»å±…çš„åˆ†æ•°
                    s_in = (pred_w_tensor * layer_scores[pred_idx_tensor]).sum()
                    
                    # åº”ç”¨è‡ªæ³¨æ„åŠ›æœºåˆ¶
                    if len(pred_idx_sample) > 1:
                        attention_scores = F.softmax(layer_scores[pred_idx_tensor], dim=0)
                        s_in = (attention_scores * pred_w_tensor * layer_scores[pred_idx_tensor]).sum()
                    else:
                        s_in = (pred_w_tensor * layer_scores[pred_idx_tensor]).sum()
                else:
                    s_in = torch.tensor(0.0, device=device)

                # åç»§é‚»å±…
                succ_idx = self.succ_indices[idx]
                succ_w = self.succ_weights[idx]

                if len(succ_idx) > 0:
                    if training and self.num_sampled_neighbors and len(succ_idx) > self.num_sampled_neighbors:
                        sampled = self.num_sampled_neighbors
                        sampled_indices = random.sample(range(len(succ_idx)), sampled)
                        succ_idx_sample = [succ_idx[i] for i in sampled_indices]
                        succ_w_sample = [succ_w[i] for i in sampled_indices]
                    else:
                        succ_idx_sample = succ_idx
                        succ_w_sample = succ_w
                    
                    succ_idx_tensor = torch.tensor(succ_idx_sample, dtype=torch.long, device=device)
                    succ_w_tensor = torch.tensor(succ_w_sample, dtype=torch.float32, device=device)
                    
                    # åŠ æƒèšåˆé‚»å±…çš„åˆ†æ•°
                    if len(succ_idx_sample) > 1:
                        attention_scores = F.softmax(layer_scores[succ_idx_tensor], dim=0)
                        s_out = (attention_scores * succ_w_tensor * layer_scores[succ_idx_tensor]).sum()
                    else:
                        s_out = (succ_w_tensor * layer_scores[succ_idx_tensor]).sum()
                else:
                    s_out = torch.tensor(0.0, device=device)

                aggregated_scores_in[idx] = s_in
                aggregated_scores_out[idx] = s_out
            # endregion
            
            # å±‚è¯„åˆ†æ›´æ–°--èšåˆåæ¿€æ´»
            combined = torch.stack([layer_scores, aggregated_scores_in, aggregated_scores_out], dim=1)  # [num_nodes, 3]
            layer_scores = self.update_fc_layers[layer](combined).squeeze()  # [num_nodes]
            
            # æ”¹è¿›çš„æ®‹å·®è¿æ¥ - ä½¿ç”¨è‡ªé€‚åº”æ®‹å·®ç³»æ•°
            layer_idx_factor = (layer + 1) / self.num_layers  # åŸºäºå±‚ç´¢å¼•çš„ç³»æ•°
            residual_alpha = 0.7 + 0.2 * layer_idx_factor  # ä»0.7é€æ¸å¢åŠ åˆ°0.9
            layer_scores = residual_alpha * layer_scores + (1 - residual_alpha) * original_scores

        # ä¸­å¿ƒæ€§è°ƒæ•´  ï¼ˆè®ºæ–‡ä¸­ä¸­å¿ƒæ€§è°ƒæ•´åªç”¨å…¥åº¦ï¼‰
        degrees = torch.tensor(
            # [self.G.in_degree(node) + self.G.out_degree(node) for node in self.nodes],
            [self.G.in_degree(node) for node in self.nodes],
            dtype=torch.float32,
            device=device
        )
        epsilon = 1e-6

        # region ä»–è‡ªå·±æ”¹è¿›çš„ä¸­å¿ƒæ€§è°ƒæ•´
        # æ”¹è¿›çš„ä¸­å¿ƒæ€§è®¡ç®— - ä½¿ç”¨è‡ªé€‚åº”ç¼©æ”¾
        degrees_scaled = degrees * self.centrality_scaling
        c_v = torch.log(degrees_scaled + epsilon)
        c_star_v = self.gamma * c_v + self.beta  # [num_nodes]

        
        # ä½¿ç”¨Tanhå¯¹æå€¼è¿›è¡Œå¹³æ»‘ï¼Œå†é€šè¿‡çº¿æ€§å˜æ¢å°†èŒƒå›´è°ƒæ•´åˆ°åˆé€‚åŒºé—´
        c_star_v_smooth = 5.0 * torch.tanh(c_star_v / 5.0)
        
        # ä½¿ç”¨å¹³æ»‘çš„è°ƒæ•´ç³»æ•°
        final_scores = torch.sigmoid(c_star_v_smooth * layer_scores)

        # åŠ¨æ€æ··åˆæƒé‡ - ä½¿ç”¨sigmoidç¡®ä¿åœ¨0-1ä¹‹é—´
        mixing_weight = torch.sigmoid(self.mixing_weight_raw)
        
        # ä½¿ç”¨åŠ¨æ€æ··åˆæƒé‡ æ··åˆæ–°æ—§åˆ†æ•°
        mixed_scores = mixing_weight * final_scores + (1 - mixing_weight) * torch.sigmoid(original_scores)
        
        # å™ªå£°æŠ‘åˆ¶ - å¦‚æœåˆ†æ•°éå¸¸ä½ï¼Œå¯èƒ½æ˜¯å™ªå£°ï¼Œå°†å…¶è¿›ä¸€æ­¥é™ä½
        noise_threshold = torch.sigmoid(self.noise_threshold)  # ç¡®ä¿é˜ˆå€¼åœ¨0-1ä¹‹é—´
        noise_mask = (mixed_scores < noise_threshold).float()
        mixed_scores = mixed_scores * (1.0 - noise_mask * 0.5)  # å¯¹ä½äºé˜ˆå€¼çš„åˆ†æ•°ä¹˜ä»¥0.5
        
        # ç¡®ä¿è¾“å‡ºå€¼ä¸¥æ ¼åœ¨0åˆ°1ä¹‹é—´ï¼ˆåº”ç”¨æˆªæ–­ï¼‰
        mixed_scores = torch.clamp(mixed_scores, 0.0, 1.0)
        # endregion
        
        # ç”¨è®ºæ–‡çš„ä¸­å¿ƒæ€§è°ƒæ•´è¦æ³¨é‡Šæ‰ä¸Šé¢regionï¼›æ”¹returnä¸ºfinal_scores
        # region è®ºæ–‡ä¸­ä¸­å¿ƒæ€§è°ƒæ•´å¾—åˆ°final_scores
        # c_v = torch.log(degrees + epsilon)  
        # c_star_v = self.gamma * c_v + self.beta  
        # final_scores = torch.sigmoid(c_star_v * layer_scores)  
        # endregion
        
        return mixed_scores


# region 2.4 æ¨¡å‹è®­ç»ƒ
def train_model(G, embeddings, labels, num_epochs=1000, learning_rate=0.0001, model=None, prev_model_path=None, validation_data=None):
    """
    è®­ç»ƒ GKCI æ¨¡å‹ï¼Œå¢å¼ºç‰ˆæœ¬æ”¯æŒæ›´å¤šè¿ç§»å­¦ä¹ åŠŸèƒ½ã€‚
    """
    start_epoch = 0
    if model is None:
        embedding_dim = embeddings.shape[1]
        hidden_dims = preset_best_param.get('hidden_dims', [64, 32])
        model = GKCIModel(
            G, 
            embedding_dim=embedding_dim,
            hidden_dims=hidden_dims,
            num_layers=preset_best_param['num_layers'],
            num_sampled_neighbors=preset_best_param['num_sampled_neighbors'],
            dropout_rate=preset_best_param.get('dropout_rate', 0.3),
            use_batch_norm=preset_best_param.get('batch_norm', True)
        )
    
    # region æœ‰å·²è®­ç»ƒçš„æ¨¡å‹(è¿ç§»å­¦ä¹ )
    prev_trained_epochs = 0
    if prev_model_path and os.path.exists(prev_model_path):
        try:
            checkpoint = torch.load(prev_model_path, weights_only=True)
            model.load_state_dict(checkpoint['model_state_dict'])       # å°†å‚æ•°åŠ è½½åˆ°æ¨¡å‹ä¸­
            prev_trained_epochs = checkpoint.get('epoch', 0)
            logging.info(f"æˆåŠŸåŠ è½½ä¸Šä¸€æ¬¡è®­ç»ƒçš„æ¨¡å‹ï¼Œä¹‹å‰å·²è®­ç»ƒ {prev_trained_epochs} è½®")
            
            # æ¢å¤å›ºå®šå‚æ•°gammaå’Œbeta
            if 'gamma' in checkpoint and 'beta' in checkpoint and not ramdom_gama_beta:
                with torch.no_grad():
                    model.gamma.copy_(torch.tensor(checkpoint['gamma']))
                    model.beta.copy_(torch.tensor(checkpoint['beta']))
                logging.info(f"ä»ä¸Šä¸€æ¬¡è®­ç»ƒåŠ è½½gamma={model.gamma.item():.4f}, beta={model.beta.item():.4f}")
                
                # æ¢å¤åŠ¨æ€å‚æ•°
                if hasattr(model, 'centrality_scaling') and 'centrality_scaling' in checkpoint:
                    model.centrality_scaling.copy_(torch.tensor(checkpoint['centrality_scaling']))
                    logging.info(f"ä»ä¸Šä¸€æ¬¡è®­ç»ƒåŠ è½½centrality_scaling={model.centrality_scaling.item():.4f}")
                
                if hasattr(model, 'mixing_weight_raw') and 'mixing_weight' in checkpoint:
                    # åç®—rawå€¼
                    mixing_weight = checkpoint['mixing_weight']
                    raw_value = torch.log(torch.tensor(mixing_weight) / (1 - torch.tensor(mixing_weight) + 1e-8))
                    model.mixing_weight_raw.copy_(raw_value)
                    logging.info(f"ä»ä¸Šä¸€æ¬¡è®­ç»ƒåŠ è½½mixing_weight={mixing_weight:.4f}")
                
                if hasattr(model, 'noise_threshold') and 'noise_threshold' in checkpoint:
                    # åç®—rawå€¼
                    noise_threshold = checkpoint['noise_threshold']
                    raw_value = torch.log(torch.tensor(noise_threshold) / (1 - torch.tensor(noise_threshold) + 1e-8))
                    model.noise_threshold.copy_(raw_value)
                    logging.info(f"ä»ä¸Šä¸€æ¬¡è®­ç»ƒåŠ è½½noise_threshold={noise_threshold:.4f}")
        except Exception as e:
            logging.error(f"åŠ è½½æ¨¡å‹å¤±è´¥: {str(e)}")
            logging.info("å°†åˆ›å»ºæ–°æ¨¡å‹è¿›è¡Œè®­ç»ƒ")
    # endregion
    
    # è®¾ç½®ç‰¹å¾å¢å¼ºå‚æ•°
    use_feature_augmentation = True  # æ˜¯å¦ä½¿ç”¨ç‰¹å¾å¢å¼º
    augmentation_level = 0.05        # åˆå§‹æ‰°åŠ¨æ°´å¹³
    augmentation_decay = 0.995       # æ¯ä¸ªepochè¡°å‡å› å­
    
    # region è‡ªé€‚åº”å­¦ä¹ ç‡ç­–ç•¥ï¼ˆè¿ç§»å­¦ä¹ ï¼‰
    if transfer_learning_settings['adaptive_learning_rate']:
        # è¿ç§»å­¦ä¹ --é™ä½å­¦ä¹ ç‡
        if enable_transfer_learning and (prev_model_path or model is not None):
            initial_lr = learning_rate * 0.3
            logging.info(f"è¿ç§»å­¦ä¹ : é™ä½åˆå§‹å­¦ä¹ ç‡è‡³ {initial_lr:.6f}")
        else:
            initial_lr = learning_rate
            
        # è¿ç§»å­¦ä¹ --ä¼˜åŒ–å™¨AdamW
        weight_decay = preset_best_param.get('weight_decay', 0.01)  # å¢åŠ æƒé‡è¡°å‡æé«˜æ³›åŒ–
        optimizer = optim.AdamW(
            # model.parameters(), 
            [p for p in model.parameters() if p.requires_grad],
            lr=initial_lr, 
            weight_decay=weight_decay,# æƒé‡è¡°å‡
            betas=(0.9, 0.999),  # æ ‡å‡†è®¾ç½®
            eps=1e-8,            # æ•°å€¼ç¨³å®šæ€§
            amsgrad=True         # ä½¿ç”¨AMSGradå˜ç§æ”¹è¿›æ”¶æ•›
        )
        
        # è¿ç§»å­¦ä¹ --å­¦ä¹ ç‡è°ƒåº¦OneCycleLR
        if (num_epochs > 0):
            scheduler = optim.lr_scheduler.OneCycleLR(
                optimizer,
                max_lr=initial_lr * 3,              # å³°å€¼å­¦ä¹ ç‡
                total_steps=num_epochs,
                pct_start=0.3,                     # å‰30%æ—¶é—´æå‡å­¦ä¹ ç‡ï¼Œå70%ä¸‹é™
                div_factor=10.0,                   # åˆå§‹å­¦ä¹ ç‡ = max_lr/10
                final_div_factor=100.0             # æœ€ç»ˆå­¦ä¹ ç‡ = max_lr/1000
            )
        else:
            scheduler = None
    else:
        # åŸå§‹æ–¹æ³• æƒé‡è¡°å‡ã€å›ºå®šå­¦ä¹ ç‡
        weight_decay = preset_best_param.get('weight_decay', 0.001)
        optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)
    # endregion
    
    # region å·²æœ‰ä¼˜åŒ–å™¨çŠ¶æ€ åŠ è½½
    # if prev_model_path and os.path.exists(prev_model_path):
    #     try:
    #         checkpoint = torch.load(prev_model_path)
    #         if 'optimizer_state_dict' in checkpoint:
    #             optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
    #             logging.info("æˆåŠŸåŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€")
    #     except Exception as e:
    #         logging.error(f"åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€å¤±è´¥: {str(e)}")
    if prev_model_path and os.path.exists(prev_model_path):
        try:
            checkpoint = torch.load(prev_model_path)
            if 'optimizer_state_dict' in checkpoint:
                # æ£€æŸ¥å‚æ•°å½¢çŠ¶æ˜¯å¦ä¸€è‡´
                model_state = model.state_dict()
                optimizer_state = checkpoint['optimizer_state_dict']
                
                mismatch = False
                for p, (k, v) in zip(model.parameters(), optimizer_state['state'].items()):
                    if p.shape != v['exp_avg'].shape:
                        logging.warning(f"å‚æ•°å½¢çŠ¶ä¸åŒ¹é…: {k} (å½“å‰{p.shape} vs ä¿å­˜çš„{v['exp_avg'].shape})")
                        mismatch = True
                
                if not mismatch:
                    optimizer.load_state_dict(optimizer_state)
                    logging.info("ä¼˜åŒ–å™¨çŠ¶æ€åŠ è½½æˆåŠŸ")
                else:
                    logging.warning("ä¼˜åŒ–å™¨çŠ¶æ€ä¸åŒ¹é…ï¼Œå°†é‡æ–°åˆå§‹åŒ–")
        except Exception as e:
            logging.error(f"åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€å¤±è´¥: {str(e)}")
    # endregion

    # å¹³è¡¡çš„æŸå¤±å‡½æ•°BCEloss - å¤„ç†ç±»åˆ«ä¸å¹³è¡¡é—®é¢˜
    criterion = nn.BCELoss()
    pos_weight = torch.tensor([(labels == 0).sum().item()/max(1, (labels == 1).sum().item())])# æ­£æ ·æœ¬æƒé‡
    criterion_weighted = nn.BCEWithLogitsLoss(pos_weight=pos_weight)        # æ­£æ ·æœ¬æŸå¤±åŠ æƒ
    
    # æ¸è¿›å¼è§£å†»ï¼ˆè¿ç§»å­¦ä¹ ï¼‰
    if transfer_learning_settings['gradual_unfreezing'] and enable_transfer_learning:
        # è§£å†»ç‚¹
        unfreeze_epochs = [num_epochs // 10, num_epochs // 5, num_epochs // 3]
        logging.info(f"è®¾ç½®æ¸è¿›å¼è§£å†»ç‚¹: {unfreeze_epochs}")

    # å°†åµŒå…¥å’Œæ ‡ç­¾è½¬æ¢ä¸ºå¼ é‡
    embeddings_tensor = torch.tensor(embeddings, dtype=torch.float32)
    labels_tensor = torch.tensor(labels, dtype=torch.float32)

    # deviceè®¾ç½®
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model.to(device)
    embeddings_tensor = embeddings_tensor.to(device)
    labels_tensor = labels_tensor.to(device)
    if transfer_learning_settings['adaptive_learning_rate']:
        pos_weight = pos_weight.to(device)
    
    # region éªŒè¯é›†è®¾ç½®
    if validation_data:
        val_embeddings, val_labels = validation_data
        val_embeddings_tensor = torch.tensor(val_embeddings, dtype=torch.float32).to(device)
        val_labels_tensor = torch.tensor(val_labels, dtype=torch.float32).to(device)
        
        # ç¡®ä¿æ¨¡å‹ç›®å½•å­˜åœ¨
        os.makedirs(model_dir, exist_ok=True)
        
        # è®¾ç½®æ—©åœæœºåˆ¶
        early_stopping = EarlyStopping(
            patience=early_stopping_patience, 
            delta=early_stopping_delta,
            path=os.path.join(model_dir, "best_model.pth"),
            verbose=True
        )
    # endregion
    
    history = {
        'loss': [],
        'accuracy': [],
        'val_loss': [],
        'val_accuracy': [],
        'epochs': [],
        'learning_rates': [],
        'augmentation_levels': []  # æ–°å¢ï¼šè®°å½•å¢å¼ºæ°´å¹³
    }
    
    # è®¡ç®—å®é™…çš„è®­ç»ƒè½®æ•°
    total_epochs = prev_trained_epochs + num_epochs     # åŠ ä¸Šæ¨¡å‹å·²ç»è®­ç»ƒçš„è½®æ•°
    logging.info(f"å°†è®­ç»ƒ {num_epochs} è½®ï¼Œè®­ç»ƒåæ€»è½®æ•°å°†è¾¾åˆ° {total_epochs}")

    # ç±»æƒé‡è®¡ç®—
    pos_counts = torch.sum(labels_tensor).item()    # æ­£æ ·æœ¬æ•°é‡
    neg_counts = len(labels_tensor) - pos_counts    # è´Ÿæ ·æœ¬æ•°é‡
    pos_weight = torch.tensor([neg_counts / max(1, pos_counts)]).to(device)     # æ­£æ ·æœ¬æƒé‡
    class_weights = torch.ones_like(labels_tensor).to(device)
    class_weights[labels_tensor == 1] = pos_weight  # æ­£æ ·æœ¬æƒé‡èµ‹å€¼

    for epoch in range(prev_trained_epochs, total_epochs): #å°±æ˜¯num_epochsçš„è½®æ•°
        # è®­ç»ƒæ¨¡å¼
        model.train()
        
        # è¿ç§»å­¦ä¹ -æ¸è¿›å¼è§£å†» - éšç€è®­ç»ƒçš„è¿›è¡Œé€æ¸è§£å†»æ›´å¤šå±‚
        if transfer_learning_settings['gradual_unfreezing'] and enable_transfer_learning:
            relative_epoch = epoch - prev_trained_epochs
            
            # æ£€æŸ¥å½“å‰è½®æ•°æ˜¯å¦åœ¨è§£å†»ç‚¹
            if relative_epoch in unfreeze_epochs:
                if relative_epoch == unfreeze_epochs[0]:
                    # è§£é”scoring_netçš„ååŠéƒ¨åˆ†
                    for layer in list(model.scoring_net.modules())[-(len(list(model.scoring_net.modules()))//2):]:
                        if isinstance(layer, nn.Linear):
                            for param in layer.parameters():
                                param.requires_grad = True
                    logging.info("è§£å†»scoring_netçš„ååŠéƒ¨åˆ†")
                
                elif relative_epoch == unfreeze_epochs[1]:
                    # è§£é”æ‰€æœ‰scoring_netå±‚
                    for param in model.scoring_net.parameters():
                        param.requires_grad = True
                    logging.info("è§£å†»æ‰€æœ‰scoring_netå±‚")
                
                elif relative_epoch == unfreeze_epochs[2]:
                    # è§£é”æ‰€æœ‰å‚æ•°
                    for param in model.parameters():
                        param.requires_grad = True
                    logging.info("è§£å†»æ‰€æœ‰æ¨¡å‹å‚æ•°")
        
        optimizer.zero_grad()
        
        # ç‰¹å¾å¢å¼º
        if use_feature_augmentation and epoch < total_epochs * 0.8:  # åªåœ¨å‰80%çš„è®­ç»ƒè¿‡ç¨‹ä¸­ä½¿ç”¨
            current_augmentation = augmentation_level * (augmentation_decay ** epoch)
            history['augmentation_levels'].append(current_augmentation)
            aug_features = augment_features(embeddings_tensor, current_augmentation)
            outputs = model(aug_features, training=True)
        else:
            outputs = model(embeddings_tensor, training=True)

        # ç¡®ä¿outputsç»“æœåœ¨æœ‰æ•ˆèŒƒå›´å†…
        # region ===ä¸ºä»€ä¹ˆè¦æˆªæ–­è€Œä¸Sigmoid
        # outputs = torch.sigmoid(outputs)

        min_train = outputs.min().item()
        max_train = outputs.max().item()
        if min_train < 0.0 or max_train > 1.0:
            logging.warning(f"è­¦å‘Šï¼šè®­ç»ƒoutputsçš„å€¼è¶…å‡ºèŒƒå›´[0,1]! min={min_train}, max={max_train}")
        outputs = torch.clamp(outputs, 0.0, 1.0)    # outputsæ¨¡å‹é¢„æµ‹ä¸ºæ­£çš„ç»“æœ
        # endregion
        
        # regionæŸå¤±è®¡ç®—
        # æ ·æœ¬ä¸å¹³è¡¡æ—¶ï¼ˆæ­£æ ·æœ¬æ•°å°‘äºæ€»æ ·æœ¬çš„30%ï¼‰ä½¿ç”¨åŠ¨æ€åŠ æƒçš„æŸå¤±å‡½æ•°
        if (labels == 1).sum().item() < len(labels) * 0.3:
            logging.info("æ£€æµ‹åˆ°æ ·æœ¬ä¸å¹³è¡¡ï¼Œä½¿ç”¨åŠ¨æ€åŠ æƒçš„æŸå¤±å‡½æ•°")
            # ptè¶Šå°ï¼Œåˆ†é”™çš„å¤šï¼›ptè¶Šå¤§ï¼Œåˆ†å¯¹çš„å¤š
            pt = outputs * labels_tensor + (1 - outputs) * (1 - labels_tensor)
            #åˆ†é”™è¶Šå¤šï¼Œæƒé‡è¶Šå¤§
            focal_weight = (1 - pt) ** 2  
            # ç»“åˆç±»åˆ«æƒé‡å’Œfocalæƒé‡
            combined_weight = class_weights * focal_weight
            
            # äºŒå…ƒäº¤å‰ç†µæŸå¤±ï¼Œç»“åˆå’Œç±»åˆ«æƒé‡å’Œfocalæƒé‡
            bce_loss = F.binary_cross_entropy(outputs, labels_tensor, reduction='none')
            loss = (bce_loss * combined_weight).mean()
        else:
            logging.info("æ ·æœ¬å¹³è¡¡ï¼Œä½¿ç”¨æ ‡å‡†BCEæŸå¤±å‡½æ•°")
            # æ ‡å‡†BCEæŸå¤±
            loss = criterion(outputs, labels_tensor)
        # endregion
        
        loss.backward()
        optimizer.step()
        
        # å­¦ä¹ ç‡æ›´æ–°
        if transfer_learning_settings['adaptive_learning_rate']:
            scheduler.step()
            current_lr = scheduler.get_last_lr()[0]
            history['learning_rates'].append(current_lr)
        
        # è®¡ç®—å‡†ç¡®ç‡accuracy
        with torch.no_grad():
            preds = (outputs > 0.5).float()         # >0.5 è§†ä¸ºæ­£ç±»ï¼Œâ‰¤0.5 è§†ä¸ºè´Ÿç±»
            correct = (preds == labels_tensor).sum().item()     # é¢„æµ‹ä¸çœŸå®æ ‡ç­¾åŒ¹é…çš„æ ·æœ¬æ•°
            accuracy = correct / len(labels_tensor)

        # å¦‚æœæœ‰éªŒè¯é›†ï¼Œè®¡ç®—éªŒè¯é›†ä¸Šçš„æ€§èƒ½
        if validation_data:
            model.eval()
            with torch.no_grad():
                val_outputs = model(val_embeddings_tensor, training=False)  #éªŒè¯é›†ç»“æœ
                
                # region ===ä¸ºä»€ä¹ˆè¦æˆªæ–­è€Œä¸Sigmoid
                # val_outputs = torch.sigmoid(val_outputs)  # å¦‚æœæ¨¡å‹è¾“å‡ºæ˜¯logitsï¼Œåˆ™éœ€è¦Sigmoid

                # ç¡®ä¿éªŒè¯è¾“å‡ºåœ¨æœ‰æ•ˆèŒƒå›´å†…
                val_outputs = torch.clamp(val_outputs, 0.0, 1.0)
                # æ£€æŸ¥å€¼æ˜¯å¦çœŸçš„åœ¨0åˆ°1ä¹‹é—´
                min_val = val_outputs.min().item()
                max_val = val_outputs.max().item()
                if min_val < 0.0 or max_val > 1.0:
                    logging.warning(f"è­¦å‘Šï¼šval_outputsçš„å€¼è¶…å‡ºèŒƒå›´[0,1]! min={min_val}, max={max_val}")
                    # å†æ¬¡ç¡®ä¿åœ¨0-1èŒƒå›´å†…
                    val_outputs = torch.clamp(val_outputs, 0.0, 1.0)
                # endregion
                
                val_loss = criterion(val_outputs, val_labels_tensor)        # éªŒè¯é›†æŸå¤±
                val_preds = (val_outputs > 0.5).float()                     # äºŒå€¼åŒ–é¢„æµ‹ç»“æœ
                val_correct = (val_preds == val_labels_tensor).sum().item() # éªŒè¯é›†æ­£ç¡®æ ·æœ¬æ•°
                val_accuracy = val_correct / len(val_labels_tensor)         # éªŒè¯é›†å‡†ç¡®ç‡
                
                # æ—©åœæ£€æŸ¥
                early_stopping(val_loss, model)
                if early_stopping.early_stop:
                    logging.info(f"æ—©åœè§¦å‘ï¼Œåœ¨ {epoch + 1} è½®åœæ­¢è®­ç»ƒ")
                    # åŠ è½½æœ€ä½³æ¨¡å‹
                    model.load_state_dict(torch.load(early_stopping.path, weights_only=True))
                    break

        # æ¯output_frequencyè½®è¾“å‡ºä¸€æ¬¡è®­ç»ƒçŠ¶æ€
        if (epoch + 1) % output_frequency == 0:
            if validation_data:
                lr_info = f", lr: {current_lr:.6f}" if transfer_learning_settings['adaptive_learning_rate'] else ""
                aug_info = f", aug: {current_augmentation:.6f}" if use_feature_augmentation and epoch < total_epochs * 0.8 else ""
                logging.info(
                    f'epoch [{epoch + 1}/{total_epochs}], loss: {loss.item():.4f}, accuracy: {accuracy * 100:.2f}%, '
                    f'val_loss: {val_loss.item():.4f}, val_accuracy: {val_accuracy * 100:.2f}%, '
                    f'gamma: {model.gamma.item():.4f}, beta: {model.beta.item():.4f}, '
                    f'centrality_scaling: {model.centrality_scaling.item():.4f}{lr_info}{aug_info}'
                )
                history['val_loss'].append(val_loss.item())
                history['val_accuracy'].append(val_accuracy)
            else:
                lr_info = f", lr: {current_lr:.6f}" if transfer_learning_settings['adaptive_learning_rate'] else ""
                logging.info(
                    f'epoch [{epoch + 1}/{total_epochs}], loss: {loss.item():.4f}, accuracy: {accuracy * 100:.2f}%, '
                    f'gamma: {model.gamma.item():.4f}, beta: {model.beta.item():.4f}, '
                    f'centrality_scaling: {model.centrality_scaling.item():.4f}{lr_info}'
                )
            
            # è®°å½•å†å²æ•°æ®
            history['loss'].append(loss.item())
            history['accuracy'].append(accuracy)
            history['epochs'].append(epoch + 1)

    return model, history, optimizer, total_epochs
# endregion

if create_hybrid:
    combined_net_file = create_combined_dataset()

if __name__ == "__main__":
    # åˆ›å»ºç¤ºä¾‹å›¾ (åœ¨å®é™…åº”ç”¨ä¸­æ›¿æ¢ä¸ºæ‚¨çš„å›¾)
    G, node_mapping = generate_sample_graph()
    
    # è¯†åˆ«å…³é”®ç±»
    key_classes, candidate_key_classes, candidate_counter = identify_key_classes(G)
    
    # æ‰“å°ç»“æœ
    print_key_class_results(G, key_classes, candidate_key_classes, candidate_counter, node_mapping)

    # è®¡ç®—èŠ‚ç‚¹åµŒå…¥  å¾—åˆ°èŠ‚ç‚¹å‘é‡(embeddings)
    embeddings = compute_node_embeddings(G, dimensions=Node2Vec_dimensions)
    
    # ä½¿ç”¨é¢å¤–çš„ä¸­å¿ƒæ€§æŒ‡æ ‡å¢å¼ºç‰¹å¾
    if use_multiple_centrality:
        centrality_features = compute_centrality_features(G)
        combined_features = combine_features(embeddings, centrality_features)
        logging.info(f"ä½¿ç”¨ç»„åˆç‰¹å¾: åµŒå…¥ç»´åº¦={embeddings.shape[1]}, ä¸­å¿ƒæ€§ç‰¹å¾ç»´åº¦={centrality_features.shape[1]}, æ€»ç»´åº¦={combined_features.shape[1]}")
    else:
        combined_features = combine_features(embeddings)
        logging.info(f"ä»…ä½¿ç”¨åµŒå…¥ç‰¹å¾: ç»´åº¦={combined_features.shape[1]}")

    
